{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### alignment comparison dependencies \n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import spacy, benepar\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import re\n",
    "from apted import APTED, apted, single_path_functions, node_indexer, resources,helpers\n",
    "from scipy import spatial\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_parse(utt):\n",
    "    try:\n",
    "        return list(nlp(utt).sents)[0]._.parse_string\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "def clean_str(string):\n",
    "    return ''.join(e for e in string if e.isalnum())\n",
    "\n",
    "\n",
    "\n",
    "def cont_tree_nodes_only(tree_parse_string):\n",
    "    \n",
    "    if tree_parse_string == '()':\n",
    "        return '()'\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        start = []\n",
    "        end = []\n",
    "        ch = ''\n",
    "        ts = ''.join(list(reversed(tree_parse_string)))\n",
    "        for i, x in enumerate(ts):\n",
    "            ch += x\n",
    "            if i != 0:\n",
    "\n",
    "                if (x != ')') and (ts[i-1] == ')'):\n",
    "                    start.append(i)\n",
    "                    count = i\n",
    "                    for y in ts[i:]:\n",
    "                        count +=1\n",
    "                        if y == ' ':\n",
    "                            end.append(count)\n",
    "                            break\n",
    "\n",
    "        cords = list(zip(start, end))\n",
    "\n",
    "\n",
    "        s = ''\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for cord in cords:\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count > 1:\n",
    "                seq_len = len(ts[cord[0]: cord[1]])\n",
    "\n",
    "                seq_ = ''.join(['_' for x in range(seq_len)])\n",
    "\n",
    "                s = s[:cord[0]] + seq_ + s[cord[1]:]\n",
    "\n",
    "            else:\n",
    "\n",
    "\n",
    "                seq_len = len(ts[cord[0]: cord[1]])\n",
    "\n",
    "                seq_ = ''.join(['_' for x in range(seq_len)])\n",
    "\n",
    "\n",
    "\n",
    "                s = ts[:cord[0]] + seq_ + ts[cord[1]:]\n",
    "\n",
    "        return ''.join(list(reversed(s.replace('_', ''))))\n",
    "    \n",
    "def remove_punct_nodes(s):\n",
    "\n",
    "    punct_li = ['(``)', '('')',  '(,)',  '(.)', '(!)', '(?)', '(:)', '(;)', '(-)',\n",
    "               '(''))', \"(''))\", '(.))', '(!))',  '(?))']\n",
    "    \n",
    "    end_punct = ['(''))', \"(''))\", '(.))', '(!))',  '(?))']\n",
    "    \n",
    "    s2= []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for x in s.split():\n",
    "        if x in end_punct:\n",
    "            count+= 1\n",
    "        if x not in punct_li:\n",
    "            s2.append(x)\n",
    "            \n",
    "    for i in range(count):\n",
    "        s2.append(')')\n",
    "    \n",
    "    return ''.join(s2)\n",
    "\n",
    "def js_syntax_convert(s):\n",
    "    return s.replace(')', ']').replace('(', '[')\n",
    "    \n",
    "\n",
    "def buildVector(iterable1, iterable2):\n",
    "    \n",
    "    counter1 = Counter(iterable1)\n",
    "    counter2= Counter(iterable2)\n",
    "    all_items = set(counter1.keys()).union( set(counter2.keys()) )\n",
    "    vector1 = [counter1[k] for k in all_items]\n",
    "    vector2 = [counter2[k] for k in all_items]\n",
    "    return vector1, vector2\n",
    "\n",
    "def pre_traversal_str(s):\n",
    "    return [x.split()[0] for x in [x.strip() for x in re.split('[()]', s) if x != ''] if x != ''][:]\n",
    "                \n",
    "                \n",
    "def tree_depth(s):\n",
    "    li = re.split('[)]', s)\n",
    "    li = [x.strip() for x in li]\n",
    "    for i, x in enumerate(li):\n",
    "        if x == '':                       \n",
    "            if set(['']) == set(li[i:]):\n",
    "\n",
    "                if len(li[i:]) == 1:\n",
    "                    return 0\n",
    "                else: \n",
    "                    if set(['']) == set(li[i:]):\n",
    "                    \n",
    "                \n",
    "                        return len(li[i:])\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "def apt_edit_tree_dist_format(tree_string):\n",
    "    return tree_string.replace('(', '{').replace(')', '}').replace(' ', '')\n",
    "\n",
    "\n",
    "def apt_edit_tree_dist_format_bt(tree_string, bracket_type1, bracket_type2):\n",
    "    return tree_string.replace(bracket_type1, '{').replace(bracket_type2, '}')\n",
    "\n",
    "\n",
    "def count_sub_trees_root(string_tree):\n",
    "    \n",
    "    string_tree = apt_edit_tree_dist_format(string_tree)\n",
    "    \n",
    "    num_sub_trees = 0\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        for i, x in enumerate(string_tree):\n",
    "\n",
    "            if (x == '}') and  (string_tree[i+1] == '}') and (string_tree[i+2] == '}'):\n",
    "                num_sub_trees += 1\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if (x == '}') and  (string_tree[i+1] == '}'):\n",
    "                    num_sub_trees += 1\n",
    "\n",
    "        return num_sub_trees\n",
    "    except:\n",
    "        return get_all_sub_trees_count(string_tree)[1]\n",
    "\n",
    "\n",
    "def get_sub_trees_from_root(str_tree):\n",
    "    \n",
    "    str_tree = apt_edit_tree_dist_format(str_tree)\n",
    "    \n",
    "    \n",
    "    sub_tree_preorder_strings = []\n",
    "    start_sub_tree_indices = []\n",
    "    end_sub_tree_indices = []\n",
    "    start_sub_tree_indices.append(0)\n",
    "    \n",
    "    try:\n",
    "        for i, x in enumerate(str_tree):\n",
    "\n",
    "\n",
    "            if (x == '}') and  (str_tree[i+1] == '}') and (str_tree[i+2] == '}'):\n",
    "                end_sub_tree_indices.append(i+1)\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if (x == '}') and  (str_tree[i+1] == '}'):\n",
    "                    end_sub_tree_indices.append(i+1)\n",
    "                    start_sub_tree_indices.append(i+1)\n",
    "\n",
    "        for cord in list(zip(start_sub_tree_indices, end_sub_tree_indices)):\n",
    "\n",
    "            sub_tree_preorder_strings.append(str_tree[cord[0]:cord[1]])\n",
    "\n",
    "        sub_tree_preorder_strings = [x.replace('}', ' ').replace('{', ' ').replace('  ', ' ').strip() for x in sub_tree_preorder_strings]\n",
    "\n",
    "        return sub_tree_preorder_strings\n",
    "    \n",
    "    except:\n",
    "        return get_all_sub_trees_count(str_tree)[0]\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "def get_all_sub_trees_count(str_tree):\n",
    "    sub_trees = [x.replace('}', ' ').replace('{', ' ').strip()  for x in apt_edit_tree_dist_format(str_tree).split('}{')]\n",
    "    num_sub_trees = len(sub_trees)\n",
    "    \n",
    "    return [sub_trees, num_sub_trees]\n",
    "\n",
    "\n",
    "def compare_all_sub_trees(tree1, tree2):\n",
    "    \n",
    "    num_shared_sub_trees = 0\n",
    "    \n",
    "    prop_sub_tree_shared = 0\n",
    "    \n",
    "    prop_sub_tree = 0\n",
    "    \n",
    "    prop_sub_trees_li = []\n",
    "    \n",
    "    tree1_preorder_subtrees = get_all_sub_trees_count(tree1)[0]\n",
    "    tree2_preorder_subtrees = get_all_sub_trees_count(tree2)[0]\n",
    "    \n",
    "    tree1_preorder_subtrees_num = get_all_sub_trees_count(tree1)[1]\n",
    "    tree2_preorder_subtrees_num = get_all_sub_trees_count(tree2)[1]\n",
    "    \n",
    "    tree1_indices = [i for i in range(len(tree1_preorder_subtrees))]\n",
    "    \n",
    "    for ix, x in enumerate(tree2_preorder_subtrees):\n",
    "        if x in tree1_preorder_subtrees:\n",
    "\n",
    "            num_shared_sub_trees += 1\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if ix in tree1_indices:\n",
    "\n",
    "                if x != tree1_preorder_subtrees[ix]:\n",
    "                    for e in x.split():\n",
    "                        if e in tree1_preorder_subtrees[ix].split():\n",
    "\n",
    "                            prop_sub_tree += 1 \n",
    "\n",
    "    \n",
    "    try:                \n",
    "        prop_sub_trees_shared = prop_sub_tree/sum([len(x.split()) for x in tree1_preorder_subtrees])\n",
    "        \n",
    "    except:\n",
    "        prop_sub_trees_shared = 0\n",
    "                     \n",
    "    \n",
    "    return [num_shared_sub_trees, prop_sub_trees_shared]\n",
    "\n",
    "\n",
    "\n",
    "def compare_root_sub_trees(tree1, tree2):\n",
    "    \n",
    "    num_shared_sub_trees = 0\n",
    "    \n",
    "    prop_sub_tree_shared = 0\n",
    "    \n",
    "    prop_sub_tree = 0\n",
    "    \n",
    "    prop_sub_trees_li = []\n",
    "    \n",
    "    tree1_preorder_subtrees = get_sub_trees_from_root(tree1)\n",
    "    tree2_preorder_subtrees = get_sub_trees_from_root(tree2)\n",
    "    \n",
    "    tree1_indices = [i for i in range(len(tree1_preorder_subtrees))]\n",
    "    \n",
    "    for ix, x in enumerate(tree2_preorder_subtrees):\n",
    "        if x in tree1_preorder_subtrees:\n",
    "\n",
    "            num_shared_sub_trees += 1\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if ix in tree1_indices:\n",
    "                if x != tree1_preorder_subtrees[ix]:\n",
    "                    for e in x.split():\n",
    "                        if e in tree1_preorder_subtrees[ix].split():\n",
    "\n",
    "                            prop_sub_tree += 1 \n",
    "                            \n",
    "                            \n",
    "    try:                \n",
    "        prop_sub_trees_shared = prop_sub_tree/sum([len(x.split()) for x in tree1_preorder_subtrees])\n",
    "        \n",
    "    except:\n",
    "        prop_sub_trees_shared = 0\n",
    "    \n",
    "    return [num_shared_sub_trees, prop_sub_trees_shared]\n",
    "    \n",
    "    \n",
    "def compare_preorder_traversal_differences(preorder_traversed_tree1, preordered_traversed_tree2):\n",
    "    \n",
    "    deviation_diff = 0\n",
    "    num_remaining_differences = 0\n",
    "    \n",
    "    for i, x in enumerate(preorder_traversed_tree1):\n",
    "        if i >= len(preordered_traversed_tree2):\n",
    "\n",
    "    \n",
    "            num_remaining_differences = len(preorder_traversed_tree1[i:])\n",
    "            \n",
    "            break\n",
    "                \n",
    "        \n",
    "        elif x == preordered_traversed_tree2[i]:\n",
    "            \n",
    "                deviation_diff -= 1\n",
    "        else:\n",
    "            deviation_diff += 1\n",
    "            \n",
    "    \n",
    "    return deviation_diff + num_remaining_differences\n",
    "\n",
    "\n",
    "def get_bases_for_entropy(tree_preoder_traversals):\n",
    "    bases = collections.Counter([tmp_base for tmp_base in tree_preoder_traversals])\n",
    "    \n",
    "    return bases\n",
    "\n",
    " \n",
    "def estimate_shannon_entropy(tree_preoder_traversal, bases):\n",
    "    \n",
    "    m = len(tree_preoder_traversal)\n",
    "    bases = bases\n",
    "\n",
    "    shannon_entropy_value = 0\n",
    "    for base in bases:\n",
    "        # number of residues\n",
    "        n_i = bases[base]\n",
    "        # n_i (# residues type i) / M (# residues in column)\n",
    "        p_i = n_i / float(m)\n",
    "        entropy_i = p_i * (math.log(p_i, 2))\n",
    "        shannon_entropy_value += entropy_i\n",
    "\n",
    "    return shannon_entropy_value * -1\n",
    "\n",
    "\n",
    "\n",
    "def compare_traversal_preorder_tree_deviation(preorder_traversed_tree1, preorder_traversed_tree2, tree1, tree2, tree2_edit_distance):\n",
    "    \n",
    "    ### num root sub trees\n",
    "    ### num all sub trees\n",
    "    ### compare all sub trees\n",
    "    ### compare root sub trees\n",
    "    ### tree order traversal\n",
    "    \n",
    "    \n",
    "    v1 = preorder_traversed_tree1\n",
    "    v2 = preorder_traversed_tree2\n",
    "    \n",
    "    nv = []\n",
    "    \n",
    "    deviation_diff = 0\n",
    "    \n",
    "    tree1_edit_distance = 0 \n",
    "    \n",
    "    tree2_edit_distance = tree2_edit_distance\n",
    "    \n",
    "    source_tree = 0\n",
    "    \n",
    "    source_tree_num_nodes = len(v1)\n",
    "    \n",
    "    target_tree_num_nodes = len(v2)\n",
    "    \n",
    "    set_diff_source_target  = len(set(v1) - (set(v2)))\n",
    "    \n",
    "    set_diff_target_source  = len(set(v2) - (set(v1)))\n",
    "    \n",
    "    tree1_num_root_sub_trees = count_sub_trees_root(tree1)\n",
    "    \n",
    "    tree1_num_all_sub_trees = get_all_sub_trees_count(tree1)[1]\n",
    "    \n",
    "    tree2_num_root_sub_trees = count_sub_trees_root(tree2)\n",
    "    \n",
    "    tree2_num_all_sub_trees = get_all_sub_trees_count(tree2)[1]\n",
    "    \n",
    "    prop_shared_all_sub_trees = compare_all_sub_trees(tree1, tree2)[1]\n",
    "    \n",
    "    prop_shared_root_sub_trees = compare_root_sub_trees(tree1, tree2)[1]\n",
    "    \n",
    "    tree1_tree2_num_shared_all_trees = compare_all_sub_trees(tree1, tree2)[0]\n",
    "    \n",
    "    tree1_tree2_num_shared_root_trees = compare_root_sub_trees(tree1, tree2)[0]\n",
    "    \n",
    "    tree2_tree1_num_shared_all_trees = compare_all_sub_trees(tree1, tree2)[0]\n",
    "    \n",
    "    tree2_tree1_num_shared_root_trees = compare_root_sub_trees(tree1, tree2)[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    symmetric_diff = len(set(v2).symmetric_difference(set(v1)))\n",
    "    \n",
    "    \n",
    "    if v1 == v2:\n",
    "        return {'deviation_diff': deviation_diff, \n",
    "                'source_tree':  source_tree,\n",
    "                'set_diff_source_target': set_diff_source_target, \n",
    "                'set_diff_target_source': set_diff_target_source,\n",
    "                'source_tree_num_nodes': source_tree_num_nodes, \n",
    "                'target_tree_num_nodes':target_tree_num_nodes,\n",
    "                'tree1_edit_distance': tree1_edit_distance,\n",
    "                'tree2_edit_distance': tree2_edit_distance,\n",
    "                'tree1_num_root_sub_trees': tree1_num_root_sub_trees,\n",
    "                'tree1_num_all_sub_trees': tree1_num_all_sub_trees,\n",
    "                'tree2_num_root_sub_trees': tree2_num_root_sub_trees,\n",
    "                'tree2_num_all_sub_trees': tree2_num_all_sub_trees,\n",
    "                'prop_shared_all_sub_trees': prop_shared_all_sub_trees,\n",
    "                'prop_shared_root_sub_trees': prop_shared_root_sub_trees,\n",
    "                'tree1_tree2_num_shared_all_trees': tree1_tree2_num_shared_all_trees,\n",
    "                'tree1_tree2_num_shared_root_trees': tree1_tree2_num_shared_root_trees,\n",
    "                'tree2_tree1_num_shared_all_trees': tree2_tree1_num_shared_all_trees,\n",
    "                'tree2_tree1_num_shared_root_trees': tree2_tree1_num_shared_root_trees\n",
    "                }\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        deviation_diff = compare_preorder_traversal_differences(v1, v2)\n",
    "     \n",
    "\n",
    "                    \n",
    "        return {'deviation_diff': deviation_diff, \n",
    "                'source_tree':  source_tree,\n",
    "                'set_diff_source_target': set_diff_source_target, \n",
    "                'set_diff_target_source': set_diff_target_source,\n",
    "                'source_tree_num_nodes': source_tree_num_nodes, \n",
    "                'target_tree_num_nodes':target_tree_num_nodes,\n",
    "                'tree1_edit_distance': tree1_edit_distance,\n",
    "                'tree2_edit_distance': tree2_edit_distance,\n",
    "                'tree1_num_root_sub_trees': tree1_num_root_sub_trees,\n",
    "                'tree1_num_all_sub_trees': tree1_num_all_sub_trees,\n",
    "                'tree2_num_root_sub_trees': tree2_num_root_sub_trees,\n",
    "                'tree2_num_all_sub_trees': tree2_num_all_sub_trees,\n",
    "                'prop_shared_all_sub_trees': prop_shared_all_sub_trees,\n",
    "                'prop_shared_root_sub_trees': prop_shared_root_sub_trees,\n",
    "                'tree1_tree2_num_shared_all_trees': tree1_tree2_num_shared_all_trees,\n",
    "                'tree1_tree2_num_shared_root_trees': tree1_tree2_num_shared_root_trees,\n",
    "                'tree2_tree1_num_shared_all_trees': tree2_tree1_num_shared_all_trees,\n",
    "                'tree2_tree1_num_shared_root_trees': tree2_tree1_num_shared_root_trees}\n",
    "    \n",
    "    \n",
    "    \n",
    "def tree_sim_dev(v1, v2, tree1, tree2, tree1_depth, tree2_depth, tree2_edit_distance):\n",
    "    \n",
    "    tree_comp = compare_traversal_preorder_tree_deviation(v1, v2, tree1, tree2, tree2_edit_distance)\n",
    "    \n",
    "    tree1_depth = [tree1_depth]\n",
    "    tree2_depth = [tree2_depth]\n",
    "    \n",
    "\n",
    "    source_tree_keys = ['source_tree', 'set_diff_source_target', 'source_tree_num_nodes', \n",
    "                        'tree2_tree1_num_shared_all_trees', 'tree2_tree1_num_shared_root_trees',\n",
    "                       'tree1_tree2_num_shared_all_trees', 'tree1_tree2_num_shared_root_trees',\n",
    "                       'tree1_num_root_sub_trees', 'tree1_num_all_sub_trees', 'tree1_edit_distance']\n",
    "    \n",
    "    target_tree_keys = ['deviation_diff', 'set_diff_target_source', 'target_tree_num_nodes',\n",
    "                       'tree1_tree2_num_shared_root_trees', 'tree1_tree2_num_shared_all_trees',\n",
    "                       'tree2_tree1_num_shared_all_trees', 'tree2_tree1_num_shared_root_trees',\n",
    "                       'tree2_num_root_sub_trees', 'tree2_num_all_sub_trees', 'tree2_edit_distance']\n",
    "    \n",
    "    source_tree_vals = [tree_comp[k] for k in source_tree_keys]\n",
    "    target_tree_vals = [tree_comp[k] for k in target_tree_keys]\n",
    "    \n",
    "    source_tree_node_type_counts = buildVector(v1, v2)[0]\n",
    "    target_tree_node_type_counts = buildVector(v1, v2)[1]\n",
    "    \n",
    "    vs = buildVector(v1, v2)[0] + tree1_depth + source_tree_vals\n",
    "    vt =  buildVector(v1, v2)[1] + tree2_depth + target_tree_vals\n",
    "\n",
    "    \n",
    "    r = 1 - spatial.distance.cosine(vs, vt)\n",
    "    \n",
    "    return {'cosine_sim': r, 'tree1_depth': tree1_depth[0], \n",
    "           'tree2_depth': tree2_depth[0],\n",
    "           'source_tree_vals': source_tree_vals, \n",
    "           'target_tree_vals': target_tree_vals,\n",
    "           'source_node_counts': source_tree_node_type_counts,\n",
    "           'target_node_counts': target_tree_node_type_counts}\n",
    "\n",
    "\n",
    "def calculate_tree_edit_distance(tree1_text, tree2_text):\n",
    "\n",
    "    t1 = helpers.Tree.from_text(tree1_text)\n",
    "    t2 = helpers.Tree.from_text(tree2_text)\n",
    "\n",
    "    ted = APTED(t1, t2)\n",
    "\n",
    "    return APTED.compute_edit_distance(ted)\n",
    "\n",
    "\n",
    "def tree_edit_sim(tree_edit_distance):\n",
    "    return 1/(1+tree_edit_distance)\n",
    "    \n",
    "\n",
    "def tree_edit_sim_sqrt(tree_edit_distance):\n",
    "    \n",
    "    return math.sqrt(1/(1+tree_edit_distance))\n",
    "\n",
    "\n",
    "def fusarolli_align_prep(token1, token2):\n",
    "\n",
    "    lemma1 = [x.lemma_ for x in nlp(token1)]\n",
    "\n",
    "    lemma2 = [x.lemma_ for x in nlp(token2)]\n",
    "\n",
    "    pos_token1 = [x.pos_ for x in nlp(token1)]\n",
    "\n",
    "    pos_token2 = [x.pos_ for x in nlp(token2)]\n",
    "\n",
    "    pos_lemma1 = [x.pos_ for x in nlp(' '.join(lemma1)) if str(x) != '-PRON-']\n",
    "\n",
    "    pos_lemma2 = [x.pos_ for x in nlp(' '.join(lemma2)) if str(x) != '-PRON-']\n",
    "    \n",
    "    return [token1, lemma1, pos_token1, pos_lemma1, token2, lemma2, pos_token2, pos_lemma2]\n",
    "\n",
    "def syntactic_lexical_sim(statement1, statement2):\n",
    "    statement1 =[x.lower().strip() for x in statement1.split()]\n",
    "    statement2 =[x.lower().strip() for x in statement2.split()]\n",
    "    \n",
    "    if (len(statement1) or len(statement2)) == 0:\n",
    "\n",
    "        return [0,0,0,0]\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        shared_lexical_items = 0\n",
    "        num_lexical_items = len(statement1) + len(statement2)\n",
    "\n",
    "        for word in statement1:\n",
    "            if word in statement2:\n",
    "                shared_lexical_items += 1\n",
    "\n",
    "        lexical_sim_ratio = (shared_lexical_items/num_lexical_items) * 2\n",
    "\n",
    "        nlp_statement1 = nlp(' '.join(statement1))\n",
    "\n",
    "        nlp_statement2 = nlp(' '.join(statement2))\n",
    "\n",
    "        statement1_pos = [x.pos_ for x in nlp_statement1]\n",
    "\n",
    "        statement2_pos = [x.pos_ for x in nlp_statement2]\n",
    "\n",
    "        shared_pos = 0\n",
    "\n",
    "        num_pos =  len(statement1_pos) + len(statement2_pos)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            for pos in statement1_pos:\n",
    "                if pos in statement2_pos:\n",
    "                    shared_pos += 1\n",
    "\n",
    "            syntactic_sim_ratio = (shared_pos/num_pos) * 2 \n",
    "\n",
    "\n",
    "            return [lexical_sim_ratio, syntactic_sim_ratio, num_lexical_items, statement2_pos]\n",
    "    \n",
    "        except:\n",
    "            return [0,0,0,0]\n",
    "        \n",
    "\n",
    "        \n",
    "def spacy_semantic_sim(utt_1, utt_2):\n",
    "    \n",
    "    return nlp(utt_1).similarity(nlp(utt_2))\n",
    "\n",
    "\n",
    "\n",
    "def lilla(statement1, statement2):\n",
    "    \n",
    "    statement1 =[x.lower().strip() for x in statement1.split()]\n",
    "    statement2 =[x.lower().strip() for x in statement2.split()]\n",
    "    \n",
    "    if (len(statement1) or len(statement2)) == 0:\n",
    "\n",
    "        return [0,0,0,0]\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        shared_lexical_items = 0\n",
    "        num_lexical_items = len(statement1) + len(statement2)\n",
    "\n",
    "        for word in statement1:\n",
    "            if word in statement2:\n",
    "                shared_lexical_items += 1\n",
    "\n",
    "        lexical_sim_ratio = (shared_lexical_items/num_lexical_items) * 2\n",
    "\n",
    "        nlp_statement1 = nlp(' '.join(statement1))\n",
    "\n",
    "        nlp_statement2 = nlp(' '.join(statement2))\n",
    "\n",
    "        statement1_pos = [x.pos_ for x in nlp_statement1]\n",
    "\n",
    "        statement2_pos = [x.pos_ for x in nlp_statement2]\n",
    "\n",
    "        shared_pos = 0\n",
    "\n",
    "        normalized_pos =  len(statement1_pos) * len(statement2_pos)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            for pos in statement1_pos:\n",
    "                if pos in statement2_pos:\n",
    "                    shared_pos += 1\n",
    "\n",
    "            syntactic_sim_ratio = (shared_pos/normalized_pos) \n",
    "\n",
    "            return [lexical_sim_ratio, syntactic_sim_ratio, num_lexical_items, statement2_pos]\n",
    "        \n",
    "        except:\n",
    "            return [0,0,0,0]\n",
    "        \n",
    "def ngram_pos(sequence1,sequence2,ngramsize=2,\n",
    "                   ignore_duplicates=True):\n",
    "    \"\"\"\n",
    "    Remove mimicked lexical sequences from two interlocutors'\n",
    "    sequences and return a dictionary of counts of ngrams\n",
    "    of the desired size for each sequence.\n",
    "    By default, consider bigrams. If desired, this may be\n",
    "    changed by setting `ngramsize` to the appropriate\n",
    "    value.\n",
    "    By default, ignore duplicate lexical n-grams when\n",
    "    processing these sequences. If desired, this may\n",
    "    be changed with `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # remove duplicates and recreate sequences\n",
    "    sequence1 = set(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = set(ngrams(sequence2,ngramsize))\n",
    "\n",
    "    # if desired, remove duplicates from sequences\n",
    "    if ignore_duplicates:\n",
    "        new_sequence1 = [tuple([''.join(pair[:]) for pair in tup]) for tup in list(sequence1 - sequence2)]\n",
    "        new_sequence2 = [tuple([''.join(pair[:]) for pair in tup]) for tup in list(sequence2 - sequence1)]\n",
    "#         print(new_sequence1, new_sequence2)\n",
    "    else:\n",
    "        new_sequence1 = [tuple([''.join(pair[:]) for pair in tup]) for tup in sequence1]\n",
    "        new_sequence2 = [tuple([''.join(pair[:]) for pair in tup]) for tup in sequence2]\n",
    "\n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)\n",
    "\n",
    "\n",
    "def ngram_lexical(sequence1,sequence2,ngramsize=2):\n",
    "    \"\"\"\n",
    "    Create ngrams of the desired size for each of two\n",
    "    interlocutors' sequences and return a dictionary\n",
    "    of counts of ngrams for each sequence.\n",
    "    By default, consider bigrams. If desired, this may be\n",
    "    changed by setting `ngramsize` to the appropriate\n",
    "    value.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate ngrams\n",
    "    sequence1 = list(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = list(ngrams(sequence2,ngramsize))\n",
    "\n",
    "    # join for counters\n",
    "    new_sequence1 = [' '.join(pair) for pair in sequence1]\n",
    "    new_sequence2 = [' '.join(pair) for pair in sequence2]\n",
    "\n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Derive cosine similarity metric, standard measure.\n",
    "    Adapted from <https://stackoverflow.com/a/33129724>.\n",
    "    \"\"\"\n",
    "\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x]**2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "\n",
    "def LexicalPOSAlignment(tok1,lem1,penn_tok1,penn_lem1,\n",
    "                             tok2,lem2,penn_tok2,penn_lem2,\n",
    "                             stan_tok1=None,stan_lem1=None,\n",
    "                             stan_tok2=None,stan_lem2=None,\n",
    "                             maxngram=2,\n",
    "                             ignore_duplicates=True,\n",
    "                             add_stanford_tags=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Derive lexical and part-of-speech alignment scores\n",
    "    between interlocutors (suffix `1` and `2` in arguments\n",
    "    passed to function).\n",
    "    By default, return scores based only on Penn POS taggers.\n",
    "    If desired, also return scores using Stanford tagger with\n",
    "    `add_stanford_tags=True` and by providing appropriate\n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and\n",
    "    `stan_lem2`.\n",
    "    By default, consider only bigram when calculating\n",
    "    similarity. If desired, this window may be expanded\n",
    "    by changing the `maxngram` argument value.\n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired,\n",
    "    duplicates may be included when calculating scores by\n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionaries for syntactic similarity\n",
    "    syntax_penn_tok = {}\n",
    "    syntax_penn_lem = {}\n",
    "\n",
    "    # if desired, generate Stanford-based scores\n",
    "    if add_stanford_tags:\n",
    "        syntax_stan_tok = {}\n",
    "        syntax_stan_lem = {}\n",
    "\n",
    "    # create empty dictionaries for lexical similarity\n",
    "    lexical_tok = {}\n",
    "    lexical_lem = {}\n",
    "\n",
    "    # cycle through all desired ngram lengths\n",
    "    for ngram in range(1,maxngram+1):\n",
    "\n",
    "        # calculate similarity for lexical ngrams (tokens and lemmas)\n",
    "        [vectorT1, vectorT2] = ngram_lexical(tok1,tok2,ngramsize=ngram)\n",
    "        [vectorL1, vectorL2] = ngram_lexical(lem1,lem2,ngramsize=ngram)\n",
    "        lexical_tok['lexical_tok{0}'.format(ngram)] = get_cosine(vectorT1,vectorT2)\n",
    "        lexical_lem['lexical_lem{0}'.format(ngram)] = get_cosine(vectorL1, vectorL2)\n",
    "\n",
    "        # calculate similarity for Penn POS ngrams (tokens)\n",
    "        [vector_penn_tok1, vector_penn_tok2] = ngram_pos(penn_tok1,penn_tok2,\n",
    "                                                ngramsize=ngram,\n",
    "                                                ignore_duplicates=ignore_duplicates)\n",
    "        syntax_penn_tok['syntax_penn_tok{0}'.format(ngram)] = get_cosine(vector_penn_tok1,\n",
    "                                                                                            vector_penn_tok2)\n",
    "        # calculate similarity for Penn POS ngrams (lemmas)\n",
    "        [vector_penn_lem1, vector_penn_lem2] = ngram_pos(penn_lem1,penn_lem2,\n",
    "                                                              ngramsize=ngram,\n",
    "                                                              ignore_duplicates=ignore_duplicates)\n",
    "        syntax_penn_lem['syntax_penn_lem{0}'.format(ngram)] = get_cosine(vector_penn_lem1,\n",
    "                                                                                            vector_penn_lem2)\n",
    "\n",
    "        # if desired, also calculate using Stanford POS\n",
    "        if add_stanford_tags:\n",
    "\n",
    "            # calculate similarity for Stanford POS ngrams (tokens)\n",
    "            [vector_stan_tok1, vector_stan_tok2] = ngram_pos(stan_tok1,stan_tok2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates)\n",
    "            syntax_stan_tok['syntax_stan_tok{0}'.format(ngram)] = get_cosine(vector_stan_tok1,\n",
    "                                                                                                vector_stan_tok2)\n",
    "\n",
    "            # calculate similarity for Stanford POS ngrams (lemmas)\n",
    "            [vector_stan_lem1, vector_stan_lem2] = ngram_pos(stan_lem1,stan_lem2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates)\n",
    "            syntax_stan_lem['syntax_stan_lem{0}'.format(ngram)] = get_cosine(vector_stan_lem1,\n",
    "                                                                                                vector_stan_lem2)\n",
    "\n",
    "    # return requested information\n",
    "    if add_stanford_tags:\n",
    "        dictionaries_list = [syntax_penn_tok, syntax_penn_lem,\n",
    "                             syntax_stan_tok, syntax_stan_lem,\n",
    "                             lexical_tok, lexical_lem]\n",
    "    else:\n",
    "        dictionaries_list = [syntax_penn_tok, syntax_penn_lem,\n",
    "                             lexical_tok, lexical_lem]\n",
    "\n",
    "    return dictionaries_list\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### eval alignment trees\n",
    "\n",
    "df = pd.read_csv('ldp_typical_kids_utts.csv') ### or ldp_atypical_kids_utts.csv\n",
    "\n",
    "df['p_tree_parse'] = [cont_parse(x) if x != '' else '()' for x in df['p_utts'].fillna('').tolist()]\n",
    "df['c_tree_parse'] = [cont_parse(x) if x != '' else '()' for x in df['c_utts'].fillna('').tolist()]\n",
    "\n",
    "df['p_tree_parse_nodes_only'] = df.p_tree_parse.map(lambda x: cont_tree_nodes_only(x))\n",
    "df['c_tree_parse_nodes_only'] = df.c_tree_parse.map(lambda x: cont_tree_nodes_only(x))\n",
    "\n",
    "df['p_tree_height'] = df['p_tree_parse_nodes_only'].map(lambda x: tree_depth(x))\n",
    "df['c_tree_height'] = df['c_tree_parse_nodes_only'].map(lambda x: tree_depth(x))\n",
    "\n",
    "df['p_tree_preorder_traversal'] = df['p_tree_parse_nodes_only'].map(lambda x: pre_traversal_str(x))\n",
    "df['c_tree_preorder_traversal'] = df['c_tree_parse_nodes_only'].map(lambda x: pre_traversal_str(x))\n",
    "\n",
    "df['p_tree_dist_format'] = df['p_tree_parse_nodes_only'].map(lambda x: apt_edit_tree_dist_format(x))\n",
    "df['c_tree_dist_format'] = df['c_tree_parse_nodes_only'].map(lambda x: apt_edit_tree_dist_format(x))\n",
    "\n",
    "\n",
    "### get speaker labels ###\n",
    "\n",
    "p_utts = df.p_utts.fillna('').tolist()\n",
    "c_utts = df.c_utts.fillna('').tolist()\n",
    "\n",
    "speaker_label = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if (p_utts[i] != '') and (c_utts[i] != ''):\n",
    "        speaker_label.append('both')\n",
    "        continue\n",
    "    \n",
    "    if (p_utts[i] != ''):\n",
    "        speaker_label.append('p')\n",
    "        continue\n",
    "        \n",
    "    elif (c_utts[i] != ''):\n",
    "        speaker_label.append('c')\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        speaker_label.append('')\n",
    "\n",
    "        \n",
    "df['speaker_label'] = speaker_label\n",
    "\n",
    "\n",
    "### label speaker shifts\n",
    "\n",
    "speaker_shift = []\n",
    "for i in range(len(df)):\n",
    "    if speaker_label[i] != speaker_label[i-1]:\n",
    "        speaker_shift.append(True)\n",
    "    else:\n",
    "        speaker_shift.append(False)\n",
    "        \n",
    "speaker_shift[0] = False ### initialize first utt as false\n",
    "\n",
    "df['speaker_shift'] = speaker_shift\n",
    "\n",
    "speaker_shift_li = df.speaker_shift.tolist()\n",
    "speaker_label_li = df.speaker_label.tolist()\n",
    "p_tree_dist_format_li = ['{}' if x == '' else x for x in df['p_tree_dist_format'].tolist()]\n",
    "c_tree_dist_format_li = ['{}' if x == '' else x for x in df['c_tree_dist_format'].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "edit_distances = []\n",
    "parent_resps = [False for i in range(len(df))]\n",
    "child_resps = [False for i in range(len(df))]\n",
    "align_opportunity = [False for i in range(len(df))]\n",
    "for i in range(len(df)):\n",
    "    print(i)\n",
    "    if speaker_shift_li[i] == True:\n",
    "        if speaker_label_li[i] == 'c':            \n",
    "            edit_distances.append(calculate_tree_edit_distance(p_tree_dist_format_li[i-1], c_tree_dist_format_li[i]))\n",
    "            align_opportunity[i-1] = True\n",
    "            align_opportunity[i] = True\n",
    "            child_resps[i] = True\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            edit_distances.append(calculate_tree_edit_distance(c_tree_dist_format_li[i-1], p_tree_dist_format_li[i]))\n",
    "            align_opportunity[i-1] = True\n",
    "            align_opportunity[i] = True\n",
    "            parent_resps[i] = True\n",
    "    else:\n",
    "        edit_distances.append(np.nan)\n",
    "        \n",
    "\n",
    "df['tree_edit_distances'] = edit_distances\n",
    "df['alignment_opportunity'] = align_opportunity\n",
    "df['child_resp'] = child_resps\n",
    "df['parent_resp'] = parent_resps\n",
    "\n",
    "tree_edit_distances_li = df.tree_edit_distances.tolist()\n",
    "p_utts_parse_tree_dist_format_li = df.p_tree_dist_format.tolist()\n",
    "c_utts_parse_tree_dist_format_li = df.c_tree_dist_format.tolist()\n",
    "parent_tree_depth_li = df.p_tree_height.tolist()\n",
    "child_tree_depth_li = df.c_tree_height.tolist()\n",
    "parent_pre_order_traversal_li = df.p_tree_preorder_traversal.tolist()\n",
    "child_pre_order_traversal_li = df.c_tree_preorder_traversal.tolist()\n",
    "\n",
    "p_utts = df.p_utts.fillna('').tolist() \n",
    "c_utts = df.c_utts.fillna('').tolist()\n",
    "parent_resp_li = df.parent_resp.tolist()\n",
    "child_resp_li = df.child_resp.tolist()\n",
    "\n",
    "comp_trees = []\n",
    "parent_resps = [False for i in range(len(df))]\n",
    "child_resps = [False for i in range(len(df))]\n",
    "align_opportunity = [False for i in range(len(df))]\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    if speaker_shift_li[i] == True:\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "\n",
    "        if (speaker_label_li[i] == 'c') and (parent_tree_depth_li[i-1] != 0):\n",
    "\n",
    "\n",
    "            count += 1\n",
    "            print(count, 'CHILD')\n",
    "            comp_trees.append(tree_sim_dev(parent_pre_order_traversal_li[i-1], child_pre_order_traversal_li[i], p_utts_parse_tree_dist_format_li[i-1], c_utts_parse_tree_dist_format_li[i], parent_tree_depth_li[i-1], child_tree_depth_li[i], tree_edit_distances_li[i]))\n",
    "\n",
    "            print(tree_sim_dev(parent_pre_order_traversal_li[i-1], child_pre_order_traversal_li[i], p_utts_parse_tree_dist_format_li[i-1], c_utts_parse_tree_dist_format_li[i], parent_tree_depth_li[i-1], child_tree_depth_li[i], tree_edit_distances_li[i]))\n",
    "\n",
    "            align_opportunity[i-1] = True\n",
    "            align_opportunity[i] = True\n",
    "            child_resps[i] = True\n",
    "\n",
    "            continue\n",
    "\n",
    "                \n",
    "        if (speaker_label_li[i] == 'p') and (child_tree_depth_li[i-1] != 0):\n",
    "            count += 1\n",
    "            print(count, 'PARENT')\n",
    "            print(i, parent_tree_depth_li[i], child_tree_depth_li[i-1])\n",
    "\n",
    "            print(tree_sim_dev(child_pre_order_traversal_li[i-1], parent_pre_order_traversal_li[i], c_utts_parse_tree_dist_format_li[i-1], p_utts_parse_tree_dist_format_li[i], child_tree_depth_li[i-1], parent_tree_depth_li[i], tree_edit_distances_li[i]))\n",
    "            comp_trees.append(tree_sim_dev(child_pre_order_traversal_li[i-1], parent_pre_order_traversal_li[i], c_utts_parse_tree_dist_format_li[i-1], p_utts_parse_tree_dist_format_li[i], child_tree_depth_li[i-1], parent_tree_depth_li[i], tree_edit_distances_li[i]))\n",
    "            align_opportunity[i-1] = True\n",
    "            align_opportunity[i] = True\n",
    "            parent_resps[i] = True\n",
    "\n",
    "\n",
    "        else:\n",
    "            count += 1\n",
    "            \n",
    "            comp_trees.append({})\n",
    "\n",
    "    else:\n",
    "        comp_trees.append({})\n",
    "        \n",
    "df['tree_comp_dicts'] = comp_trees\n",
    "\n",
    "df['ldp_tree_sim'] = df['tree_comp_dicts'].map(lambda x: x.get('cosine_sim'))\n",
    "\n",
    "df['tree_edit_sim'] = df['tree_edit_distances'].map(lambda x: tree_edit_sim(x))\n",
    "df['tree_edit_sim_sqrt'] = df['tree_edit_distances'].map(lambda x: tree_edit_sim_sqrt(x))\n",
    "\n",
    "### EXPORT TO CSV ###\n",
    "\n",
    "\n",
    "df.to_csv('TYPICAL_ldp_children_tree_comparison_data_1_25_22.csv') ### or ATYPICAL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in dfs \n",
    "\n",
    "aty_df = pd.read_csv('ATYPICAL_ldp_children_tree_comparison_data_1_28_22.csv')\n",
    "\n",
    "typ_df = pd.read_csv('TYPICAL_ldp_children_tree_comparison_data_1_25_22.csv')\n",
    "\n",
    "\n",
    "### find one word c_utts ###\n",
    "\n",
    "c_utts = typ_df.c_utts.fillna('').tolist()\n",
    "typ_one_word = []\n",
    "cs = [i for i, x in enumerate (typ_df.child_resps.tolist())]\n",
    "for i in cs:\n",
    "    if len(c_utts[i].strip().split()) == 1:\n",
    "        typ_one_word.append(True)\n",
    "    else:\n",
    "        typ_one_word.append(False)\n",
    "        \n",
    "        \n",
    "c_utts = aty_df.c_utts.fillna('').tolist()\n",
    "aty_one_word = []\n",
    "cs = [i for i, x in enumerate (aty_df.child_resps.tolist())]\n",
    "for i in cs:\n",
    "    if len(c_utts[i].strip().split()) == 1:\n",
    "        aty_one_word.append(True)\n",
    "    else:\n",
    "        aty_one_word.append(False)\n",
    "        \n",
    "        \n",
    "typ_df['one_word_bool'] = typ_one_word\n",
    "\n",
    "aty_df['one_word_bool'] = aty_one_word\n",
    "\n",
    "\n",
    "### TYP / TYPICAL DF ONLY FROM HERE ON FUSAROLI ###\n",
    "\n",
    "\n",
    "speaker_shifts = typ_df.speaker_shift.tolist()\n",
    "speaker_labels = typ_df.speaker_label.tolist()\n",
    "p_utts = typ_df.p_utts.fillna('').tolist()\n",
    "c_utts = typ_df.c_utts.fillna('').tolist()\n",
    "\n",
    "fusarolli_align_li = []\n",
    "pcount = 0\n",
    "ccount = 0\n",
    "ncount = 0\n",
    "syntactic_lexical_sim_li = []\n",
    "lilla_li  = []\n",
    "\n",
    "for i, x in enumerate(speaker_shifts):\n",
    "    if (x == True) and (speaker_labels[i] == 'p'):\n",
    "        pcount += 1\n",
    "        syntactic_lexical_sim_li.append(syntactic_lexical_sim(p_utts[i], c_utts[i-1]))\n",
    "        lilla_li.append(lilla(p_utts[i], c_utts[i-1]))\n",
    "        prep_data = fusarolli_align_prep(p_utts[i], c_utts[i-1])\n",
    "        token1 = prep_data[0]\n",
    "        lemma1 = prep_data[1]\n",
    "        pos_tok1 =  prep_data[2]\n",
    "        pos_lemma1 = prep_data[3]\n",
    "        token2 = prep_data[4]\n",
    "        lemma2 = prep_data[5]\n",
    "        pos_tok2 = prep_data[6]\n",
    "        pos_lemma2 = prep_data[7]\n",
    "        fusarolli_align_li.append(LexicalPOSAlignment(token1, lemma1, pos_tok1, pos_lemma1, token2, lemma2, pos_tok2, pos_lemma2, ignore_duplicates=False))\n",
    "        \n",
    "    \n",
    "    if (x == True) and (speaker_labels[i] == 'c'):\n",
    "        ccount += 1\n",
    "        syntactic_lexical_sim_li.append(syntactic_lexical_sim(c_utts[i], p_utts[i-1]))\n",
    "        lilla_li.append(lilla(c_utts[i], p_utts[i-1]))\n",
    "        prep_data = fusarolli_align_prep(c_utts[i], p_utts[i-1])\n",
    "        token1 = prep_data[0]\n",
    "        lemma1 = prep_data[1]\n",
    "        pos_tok1 =  prep_data[2]\n",
    "        pos_lemma1 = prep_data[3]\n",
    "        token2 = prep_data[4]\n",
    "        lemma2 = prep_data[5]\n",
    "        pos_tok2 = prep_data[6]\n",
    "        pos_lemma2 = prep_data[7]\n",
    "        fusarolli_align_li.append(LexicalPOSAlignment(token1, lemma1, pos_tok1, pos_lemma1, token2, lemma2, pos_tok2, pos_lemma2, ignore_duplicates=False))\n",
    "        \n",
    "    if (x == True) and (speaker_labels[i] == 'both'):\n",
    "        ccount += 1\n",
    "        prep_data = fusarolli_align_prep(c_utts[i], p_utts[i-1])\n",
    "        token1 = prep_data[0]\n",
    "        lemma1 = prep_data[1]\n",
    "        pos_tok1 =  prep_data[2]\n",
    "        pos_lemma1 = prep_data[3]\n",
    "        token2 = prep_data[4]\n",
    "        lemma2 = prep_data[5]\n",
    "        pos_tok2 = prep_data[6]\n",
    "        pos_lemma2 = prep_data[7]\n",
    "        fusarolli_align_li.append(LexicalPOSAlignment(token1, lemma1, pos_tok1, pos_lemma1, token2, lemma2, pos_tok2, pos_lemma2, ignore_duplicates=False))\n",
    "        \n",
    "    else:\n",
    "        syntactic_lexical_sim_li.append([])\n",
    "        lilla_li.append([])\n",
    "        fusarolli_align_li.append([])\n",
    "        ncount += 1 \n",
    "        \n",
    "        \n",
    "typ_df['syntactic_lexical_sim_li'] = syntactic_lexical_sim_li\n",
    "typ_df['lilla_li'] = lilla_li\n",
    "typ_df['fusarolli_align_li'] = fusarolli_align_li\n",
    "\n",
    "\n",
    "\n",
    "### EXPORT TO CSV\n",
    "\n",
    "typ_df.to_csv('typ_df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATYP / ATYPICAL DF ONLY FROM HERE ON FUSAROLI\n",
    "\n",
    "\n",
    "speaker_shifts = aty_df.speaker_shift.tolist()\n",
    "speaker_labels = aty_df.speaker_label.tolist()\n",
    "p_utts = aty_df.p_utts.fillna('').tolist()\n",
    "c_utts = aty_df.c_utts.fillna('').tolist()\n",
    "parent_tree_depth_li = aty_df.p_tree_height.tolist()\n",
    "child_tree_depth_li = aty_df.c_tree_height.tolist()\n",
    "\n",
    "\n",
    "fusarolli_align_li = []\n",
    "pcount = 0\n",
    "ccount = 0\n",
    "ncount = 0\n",
    "syntactic_lexical_sim_li = []\n",
    "lilla_li  = []\n",
    "\n",
    "for i in range(len(aty_df)):\n",
    "    \n",
    "    if speaker_shifts[i] == True:\n",
    "        \n",
    "        count = 0\n",
    "\n",
    "        if (speaker_labels[i] == 'c') and (parent_tree_depth_li[i-1] != 0):\n",
    "        \n",
    "            syntactic_lexical_sim_li.append(syntactic_lexical_sim(p_utts[i-1], c_utts[i]))\n",
    "            lilla_li.append(lilla(p_utts[i-1], c_utts[i]))\n",
    "            prep_data = fusarolli_align_prep(p_utts[i], c_utts[i])\n",
    "            token1 = prep_data[0]\n",
    "            lemma1 = prep_data[1]\n",
    "            pos_tok1 =  prep_data[2]\n",
    "            pos_lemma1 = prep_data[3]\n",
    "            token2 = prep_data[4]\n",
    "            lemma2 = prep_data[5]\n",
    "            pos_tok2 = prep_data[6]\n",
    "            pos_lemma2 = prep_data[7]\n",
    "            fusarolli_align_li.append(LexicalPOSAlignment(token1, lemma1, pos_tok1, pos_lemma1, token2, lemma2, pos_tok2, pos_lemma2, ignore_duplicates=False))\n",
    "\n",
    "            continue\n",
    "\n",
    "                \n",
    "        if (speaker_labels[i] == 'p') and (child_tree_depth_li[i-1] != 0):\n",
    "            \n",
    "            syntactic_lexical_sim_li.append(syntactic_lexical_sim(c_utts[i-1], p_utts[i]))\n",
    "            lilla_li.append(lilla(c_utts[i-1], p_utts[i]))\n",
    "            prep_data = fusarolli_align_prep(c_utts[i-1], p_utts[i])\n",
    "            token1 = prep_data[0]\n",
    "            lemma1 = prep_data[1]\n",
    "            pos_tok1 =  prep_data[2]\n",
    "            pos_lemma1 = prep_data[3]\n",
    "            token2 = prep_data[4]\n",
    "            lemma2 = prep_data[5]\n",
    "            pos_tok2 = prep_data[6]\n",
    "            pos_lemma2 = prep_data[7]\n",
    "            fusarolli_align_li.append(LexicalPOSAlignment(token1, lemma1, pos_tok1, pos_lemma1, token2, lemma2, pos_tok2, pos_lemma2, ignore_duplicates=False))\n",
    "\n",
    "        else:\n",
    "            fusarolli_align_li.append([])\n",
    "            syntactic_lexical_sim_li.append([])\n",
    "            lilla_li.append([])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        fusarolli_align_li.append([])\n",
    "        syntactic_lexical_sim_li.append([])\n",
    "        lilla_li.append([])\n",
    "        \n",
    "aty_df['syntactic_lexical_sim_li'] = syntactic_lexical_sim_li\n",
    "aty_df['lilla_li'] = lilla_li\n",
    "aty_df['fusarolli_align_li'] = fusarolli_align_li\n",
    "\n",
    "\n",
    "### EXPORT TO CSV\n",
    "\n",
    "typ_df.to_csv('aty_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aty_df['ldp_lexical_sim'] = [x[0] if len(x) != 0 else np.nan for x in aty_df['lilla_li'].tolist()] \n",
    "aty_df['ldp_syntactic_sim']  = [x[1] if len(x) != 0 else np.nan for x in aty_df['lilla_li'].tolist()] \n",
    "\n",
    "aty_df['fusarolli_align_syntax_penn_tok1'] = [x[0].get('syntax_penn_tok1') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "aty_df['fusarolli_align_syntax_penn_tok2']  = [x[0].get('syntax_penn_tok2') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "aty_df['fusarolli_align_syntax_penn_lemma1'] = [x[1].get('syntax_penn_lem1') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "aty_df['fusarolli_align_syntax_penn_lemma2']  = [x[1].get('syntax_penn_lem2') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "aty_df['fusarolli_align_lexical_penn_tok1'] = [x[2].get('lexical_tok1') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "aty_df['fusarolli_align_lexical_penn_tok2']  = [x[2].get('lexical_tok2') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "aty_df['fusarolli_align_lexical_penn_lemma1'] = [x[3].get('lexical_lem1') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "aty_df['fusarolli_align_lexical_penn_lemma2']  = [x[3].get('lexical_lem2') if len(x) != 0 else np.nan for x in aty_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "aty_df['ldp_lexical_sim_OLD'] = [x[0] if len(x) != 0 else np.nan for x in aty_df['syntactic_lexical_sim_li'].tolist()] \n",
    "aty_df['ldp_syntactic_sim_OLD']  = [x[1] if len(x) != 0 else np.nan for x in aty_df['syntactic_lexical_sim_li'].tolist()] \n",
    "\n",
    "typ_df['ldp_lexical_sim'] = [x[0] if len(x) != 0 else np.nan for x in typ_df['lilla_li'].tolist()] \n",
    "typ_df['ldp_syntactic_sim']  = [x[1] if len(x) != 0 else np.nan for x in typ_df['lilla_li'].tolist()]\n",
    "\n",
    "typ_df['ldp_lexical_sim'] = [x[0] if len(x) != 0 else np.nan for x in typ_df['lilla_li'].tolist()] \n",
    "typ_df['ldp_syntactic_sim']  = [x[1] if len(x) != 0 else np.nan for x in typ_df['lilla_li'].tolist()] \n",
    "\n",
    "typ_df['fusarolli_align_syntax_penn_tok1'] = [x[0].get('syntax_penn_tok1') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "typ_df['fusarolli_align_syntax_penn_tok2']  = [x[0].get('syntax_penn_tok2') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "typ_df['fusarolli_align_syntax_penn_lemma1'] = [x[1].get('syntax_penn_lem1') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "typ_df['fusarolli_align_syntax_penn_lemma2']  = [x[1].get('syntax_penn_lem2') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "typ_df['fusarolli_align_lexical_penn_tok1'] = [x[2].get('lexical_tok1') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "typ_df['fusarolli_align_lexical_penn_tok2']  = [x[2].get('lexical_tok2') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "typ_df['fusarolli_align_lexical_penn_lemma1'] = [x[3].get('lexical_lem1') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "typ_df['fusarolli_align_lexical_penn_lemma2']  = [x[3].get('lexical_lem2') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "typ_df['ldp_lexical_sim_OLD'] = [x[0] if len(x) != 0 else np.nan for x in typ_df['syntactic_lexical_sim_li'].tolist()] \n",
    "typ_df['ldp_syntactic_sim_OLD']  = [x[1] if len(x) != 0 else np.nan for x in typ_df['syntactic_lexical_sim_li'].tolist()]\n",
    "\n",
    "typ_df['fusarolli_align_lexical_penn_tok1'] = [x[2].get('lexical_tok1') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "typ_df['fusarolli_align_lexical_penn_tok2']  = [x[2].get('lexical_tok2') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "\n",
    "typ_df['fusarolli_align_lexical_penn_lemma1'] = [x[3].get('lexical_lem1') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n",
    "typ_df['fusarolli_align_lexical_penn_lemma2']  = [x[3].get('lexical_lem2') if len(x) != 0 else np.nan for x in typ_df['fusarolli_align_li'].tolist()] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORT TO CSVS\n",
    "\n",
    "typ_df.to_csv('ldp_typical_children_tree_comparison_w_ldp_and_fusarolli_lexical_syntactic_alignment_1_30_2022.csv')\n",
    "\n",
    "aty_df.to_csv('ldp_atypical_children_tree_comparison_w_ldp_and_fusarolli_lexical_syntactic_alignment_1_30_2022.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_rand_baseline_resp_avgs = {}\n",
    "\n",
    "count = 0\n",
    "\n",
    "for sub in aty_subs:\n",
    "    permuted_rand_baseline_resp_avgs[sub] = {}\n",
    "    for s in aty_sessions:\n",
    "        permuted_rand_baseline_resp_avgs[sub][s] = {}\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    \n",
    "        cp_fusarolli_align_li = []\n",
    "        pc_fusarolli_align_li = []\n",
    "        print(sub, s)\n",
    "        p_rand_utts = get_p_resp_utts[sub][s]['p_rand_resp_utts']\n",
    "        c_rand_utts = get_c_resp_utts[sub][s]['c_rand_resp_utts']\n",
    "\n",
    "        p_rand_utts = [x for x in p_rand_utts if x != '']\n",
    "        c_rand_utts = [x for x in c_rand_utts if x != '']\n",
    "\n",
    "        if (len(p_rand_utts) == 0) or (len(c_rand_utts) == 0):\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            if len(p_rand_utts) > len(c_rand_utts):\n",
    "                lim = len(c_rand_utts)\n",
    "\n",
    "            else:\n",
    "                lim = len(p_rand_utts)\n",
    "\n",
    "            child_cont_trees = [cont_parse(x) for x in c_rand_utts]\n",
    "            parent_cont_trees = [cont_parse(x) for x in p_rand_utts]\n",
    "            parent_tree_depths = [tree_depth(x) for x in parent_cont_trees]\n",
    "            child_tree_depths = [tree_depth(x) for x in child_cont_trees]\n",
    "            child_tree_edit_format = [apt_edit_tree_dist_format(cont_tree_nodes_only(x)) for x in child_cont_trees]\n",
    "            parent_tree_edit_format = [apt_edit_tree_dist_format(cont_tree_nodes_only(x)) for x in parent_cont_trees]\n",
    "\n",
    "            print(len(child_tree_edit_format), len(parent_tree_edit_format))\n",
    "\n",
    "            parent_child_edit_dists = [calculate_tree_edit_distance(child_tree_edit_format[i], parent_tree_edit_format[i-1]) for i in range(lim)]\n",
    "            child_parent_edit_dists = [calculate_tree_edit_distance(parent_tree_edit_format[i], child_tree_edit_format[i-1]) for i in range(lim)]\n",
    "\n",
    "            ### tree edit_avgs ### \n",
    "\n",
    "            print(len(child_parent_edit_dists), len(p_rand_utts), len(c_rand_utts))\n",
    "\n",
    "            child_parent_tree_edit_sim_sqrt_ave = sum([tree_edit_sim_sqrt(x) for x in child_parent_edit_dists])/lim\n",
    "            parent_child_tree_edit_sim_sqrt_ave = sum([tree_edit_sim_sqrt(x) for x in parent_child_edit_dists])/lim\n",
    "            child_parent_tree_edit_sim_ave = sum([tree_edit_sim(x) for x in child_parent_edit_dists])/lim\n",
    "            parent_child_tree_edit_sim_ave = sum([tree_edit_sim(x) for x in parent_child_edit_dists])/lim\n",
    "\n",
    "            ### end tree edit avgs ###\n",
    "\n",
    "            ### start pre_traversal analysis ###\n",
    "\n",
    "            parent_preorder_traversals = [pre_traversal_str(cont_tree_nodes_only(x)) for x in parent_cont_trees]\n",
    "            child_preorder_traversals = [pre_traversal_str(cont_tree_nodes_only(x)) for x in child_cont_trees]\n",
    "\n",
    "            tree_sim_dev_child_parent = sum([tree_sim_dev(child_preorder_traversals[i], parent_preorder_traversals[i], child_tree_edit_format[i], parent_tree_edit_format[i], child_tree_depths[i], parent_tree_depths[i], parent_child_edit_dists[i])['cosine_sim'] for i in range(lim)])/lim\n",
    "\n",
    "            tree_sim_dev_parent_child = sum([tree_sim_dev(parent_preorder_traversals[i], child_preorder_traversals[i], parent_tree_edit_format[i], child_tree_edit_format[i], parent_tree_depths[i], child_tree_depths[i], child_parent_edit_dists[i])['cosine_sim'] for i in range(lim)])/lim\n",
    "\n",
    "            print(child_parent_tree_edit_sim_sqrt_ave, parent_child_tree_edit_sim_sqrt_ave, child_parent_tree_edit_sim_ave, parent_child_tree_edit_sim_ave)\n",
    "\n",
    "            print(tree_sim_dev_child_parent, tree_sim_dev_parent_child)\n",
    "\n",
    "\n",
    "\n",
    "            ### end pre_traversal analysis ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ### start fusarolli ldp syntax, lexical, semantic data analysis ###\n",
    "\n",
    "\n",
    "            child_parent_lilla = [lilla(c_rand_utts[i], p_rand_utts[i-1]) for i in range(lim)]\n",
    "            parent_child_lilla = [lilla(p_rand_utts[i], c_rand_utts[i-1]) for i in range(lim)]\n",
    "\n",
    "            child_parent_old_ldp = [syntactic_lexical_sim(c_rand_utts[i], p_rand_utts[i-1]) for i in range(lim)]\n",
    "\n",
    "            parent_child_old_ldp = [syntactic_lexical_sim(p_rand_utts[i], c_rand_utts[i-1]) for i in range(lim)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            child_parent_lilla_li_lex = [x[0] for x in child_parent_lilla]\n",
    "            parent_child_lilla_li_lex = [x[0] for x in parent_child_lilla]\n",
    "\n",
    "            child_parent_lilla_li_syntax = [x[1] for x in child_parent_lilla]\n",
    "            parent_child_lilla_li_syntax = [x[1] for x in parent_child_lilla]\n",
    "\n",
    "            child_parent_lilla_li_lex_ave = sum(child_parent_lilla_li_lex)/lim\n",
    "\n",
    "            parent_child_lilla_li_lex_ave = sum(parent_child_lilla_li_lex)/lim\n",
    "\n",
    "            child_parent_lilla_li_syntax_ave = sum(child_parent_lilla_li_syntax)/lim\n",
    "\n",
    "            parent_child_lilla_li_syntax_ave = sum(parent_child_lilla_li_syntax)/lim\n",
    "\n",
    "\n",
    "            child_parent_old_ldp_li_lex = [x[0] for x in child_parent_old_ldp] \n",
    "            parent_child_old_ldp_li_lex = [x[0] for x in parent_child_old_ldp]\n",
    "\n",
    "            child_parent_old_ldp_li_syntax = [x[1] for x in child_parent_old_ldp] \n",
    "            parent_child_old_ldp_li_syntax = [x[1] for x in parent_child_old_ldp]\n",
    "\n",
    "            child_parent_old_ldp_li_lex_ave = sum(child_parent_old_ldp_li_lex)/lim\n",
    "\n",
    "\n",
    "            parent_child_old_ldp_li_lex_ave = sum(parent_child_old_ldp_li_lex)/lim\n",
    "\n",
    "            child_parent_old_ldp_li_syntax_ave = sum(child_parent_old_ldp_li_syntax)/lim\n",
    "\n",
    "            parent_child_old_ldp_li_syntax_ave = sum(parent_child_old_ldp_li_syntax)/lim\n",
    "\n",
    "            print(child_parent_lilla_li_lex_ave, parent_child_lilla_li_lex_ave)\n",
    "\n",
    "            print(child_parent_lilla_li_syntax_ave, parent_child_lilla_li_syntax_ave)\n",
    "\n",
    "            print(child_parent_old_ldp_li_lex_ave, parent_child_old_ldp_li_lex_ave)\n",
    "\n",
    "            print(child_parent_old_ldp_li_syntax_ave, parent_child_old_ldp_li_syntax_ave)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            child_parent_prep_data = [fusarolli_align_prep(p_rand_utts[i], c_rand_utts[i-1]) for i in range(lim)]\n",
    "            parent_child_prep_data = [fusarolli_align_prep(c_rand_utts[i], p_rand_utts[i-1]) for i in range(lim)]\n",
    "\n",
    "            cp_token1 = [x[0] for x in child_parent_prep_data]\n",
    "            cp_lemma1 = [x[1] for x in child_parent_prep_data]\n",
    "            cp_pos_tok1 = [x[2] for x in child_parent_prep_data]\n",
    "            cp_pos_lemma1 = [x[3] for x in child_parent_prep_data]\n",
    "            cp_token2 = [x[4] for x in child_parent_prep_data]\n",
    "            cp_lemma2 = [x[5] for x in child_parent_prep_data]\n",
    "            cp_pos_tok2 = [x[6] for x in child_parent_prep_data]\n",
    "            cp_pos_lemma2 = [x[7] for x in child_parent_prep_data]\n",
    "\n",
    "            cp_fusarolli_align_li = [LexicalPOSAlignment(cp_token1[i], cp_lemma1[i], cp_pos_tok1[i], cp_pos_lemma1[i], cp_token2[i], cp_lemma2[i], cp_pos_tok2[i], cp_pos_lemma2[i], ignore_duplicates=False) for i in range(lim)]\n",
    "\n",
    "            cp_fusarolli_align_syntax_penn_tok1_ave = sum([x[0].get('syntax_penn_tok1') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li])/len(cp_fusarolli_align_li) \n",
    "            cp_fusarolli_align_syntax_penn_tok2_ave = sum([x[0].get('syntax_penn_tok2') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li]) /len(cp_fusarolli_align_li) \n",
    "\n",
    "            cp_fusarolli_align_syntax_penn_lemma1_ave = sum([x[1].get('syntax_penn_lem1') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li])/len(cp_fusarolli_align_li)\n",
    "            cp_fusarolli_align_syntax_penn_lemma2_ave  = sum([x[1].get('syntax_penn_lem2') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li]) / len(cp_fusarolli_align_li)\n",
    "\n",
    "            cp_fusarolli_align_lexical_penn_tok1_ave = sum([x[2].get('lexical_tok1') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li]) / len(cp_fusarolli_align_li)\n",
    "            cp_fusarolli_align_lexical_penn_tok2_ave  = sum([x[2].get('lexical_tok2') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li]) / len(cp_fusarolli_align_li)\n",
    "\n",
    "            cp_fusarolli_align_lexical_penn_lemma1_ave = sum([x[3].get('lexical_lem1') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li]) / len(cp_fusarolli_align_li)\n",
    "            cp_fusarolli_align_lexical_penn_lemma2_ave = sum([x[3].get('lexical_lem2') if len(x) != 0 else np.nan for x in cp_fusarolli_align_li]) / len(cp_fusarolli_align_li)\n",
    "\n",
    "\n",
    "\n",
    "            pc_token1 = [x[0] for x in parent_child_prep_data]\n",
    "            pc_lemma1 = [x[1] for x in parent_child_prep_data]\n",
    "            pc_pos_tok1 = [x[2] for x in parent_child_prep_data]\n",
    "            pc_pos_lemma1 = [x[3] for x in parent_child_prep_data]\n",
    "            pc_token2 = [x[4] for x in parent_child_prep_data]\n",
    "            pc_lemma2 = [x[5] for x in parent_child_prep_data]\n",
    "            pc_pos_tok2 = [x[6] for x in parent_child_prep_data]\n",
    "            pc_pos_lemma2 = [x[7] for x in parent_child_prep_data]\n",
    "\n",
    "            pc_fusarolli_align_li = [LexicalPOSAlignment(pc_token1[i], pc_lemma1[i], pc_pos_tok1[i], pc_pos_lemma1[i], pc_token2[i], pc_lemma2[i], pc_pos_tok2[i], pc_pos_lemma2[i], ignore_duplicates=False) for i in range(lim)]\n",
    "\n",
    "\n",
    "\n",
    "            pc_fusarolli_align_syntax_penn_tok1_ave = sum([x[0].get('syntax_penn_tok1') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li])/len(pc_fusarolli_align_li) \n",
    "            pc_fusarolli_align_syntax_penn_tok2_ave = sum([x[0].get('syntax_penn_tok2') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li]) /len(pc_fusarolli_align_li) \n",
    "\n",
    "            pc_fusarolli_align_syntax_penn_lemma1_ave = sum([x[1].get('syntax_penn_lem1') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li])/len(pc_fusarolli_align_li)\n",
    "            pc_fusarolli_align_syntax_penn_lemma2_ave  = sum([x[1].get('syntax_penn_lem2') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li]) / len(pc_fusarolli_align_li)\n",
    "\n",
    "            pc_fusarolli_align_lexical_penn_tok1_ave = sum([x[2].get('lexical_tok1') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li]) / len(pc_fusarolli_align_li)\n",
    "            pc_fusarolli_align_lexical_penn_tok2_ave  = sum([x[2].get('lexical_tok2') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li]) / len(pc_fusarolli_align_li)\n",
    "\n",
    "            pc_fusarolli_align_lexical_penn_lemma1_ave = sum([x[3].get('lexical_lem1') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li]) / len(pc_fusarolli_align_li)\n",
    "            pc_fusarolli_align_lexical_penn_lemma2_ave = sum([x[3].get('lexical_lem2') if len(x) != 0 else np.nan for x in pc_fusarolli_align_li]) / len(pc_fusarolli_align_li)\n",
    "\n",
    "            print('FUSAROLI')\n",
    "\n",
    "            print( pc_fusarolli_align_syntax_penn_tok1_ave,  \n",
    "            pc_fusarolli_align_syntax_penn_tok2_ave,\n",
    "            pc_fusarolli_align_syntax_penn_lemma1_ave,\n",
    "            pc_fusarolli_align_syntax_penn_lemma2_ave, \n",
    "            pc_fusarolli_align_lexical_penn_tok1_ave,\n",
    "            pc_fusarolli_align_lexical_penn_tok2_ave,\n",
    "            pc_fusarolli_align_lexical_penn_lemma1_ave, \n",
    "            pc_fusarolli_align_lexical_penn_lemma2_ave,\n",
    "\n",
    "            cp_fusarolli_align_syntax_penn_tok1_ave,  \n",
    "            cp_fusarolli_align_syntax_penn_tok2_ave,\n",
    "            cp_fusarolli_align_syntax_penn_lemma1_ave,\n",
    "            cp_fusarolli_align_syntax_penn_lemma2_ave, \n",
    "            cp_fusarolli_align_lexical_penn_tok1_ave,\n",
    "            cp_fusarolli_align_lexical_penn_tok2_ave,\n",
    "            cp_fusarolli_align_lexical_penn_lemma1_ave, \n",
    "            cp_fusarolli_align_lexical_penn_lemma2_ave)\n",
    "\n",
    "            ## end fusarolli ldp syntax, lexical, semantic data analysis ###\n",
    "\n",
    "\n",
    "            ### start semantic simalirity scores ###\n",
    "\n",
    "            rand_semantic_sim_ave = sum([nlp(c_rand_utts[i]).similarity(nlp(p_rand_utts[i])) for i in range(lim)])/lim\n",
    "\n",
    "            ### end semantic similarity scores ###\n",
    "\n",
    "\n",
    "            #### ADD to dictionary ####\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['child_parent_tree_edit_sim_sqrt_ave'] = child_parent_tree_edit_sim_sqrt_ave \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['parent_child_tree_edit_sim_sqrt_ave'] = parent_child_tree_edit_sim_sqrt_ave \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['child_parent_tree_edit_sim_ave'] = child_parent_tree_edit_sim_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['parent_child_tree_edit_sim_ave'] = parent_child_tree_edit_sim_ave\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['tree_sim_dev_child_parent'] = tree_sim_dev_child_parent\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['tree_sim_dev_parent_child'] = tree_sim_dev_parent_child\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['child_parent_lilla_li_lex_ave'] = child_parent_lilla_li_lex_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['parent_child_lilla_li_lex_ave'] = parent_child_lilla_li_lex_ave\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['child_parent_lilla_li_syntax_ave'] = child_parent_lilla_li_syntax_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['parent_child_lilla_li_syntax_ave'] = parent_child_lilla_li_syntax_ave\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['child_parent_old_ldp_li_lex_ave'] = child_parent_old_ldp_li_lex_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['parent_child_old_ldp_li_lex_ave'] = parent_child_old_ldp_li_lex_ave\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['child_parent_old_ldp_li_syntax_ave'] = child_parent_old_ldp_li_syntax_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['parent_child_old_ldp_li_syntax_ave'] = parent_child_old_ldp_li_syntax_ave\n",
    "\n",
    "\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_syntax_penn_tok1_ave'] = pc_fusarolli_align_syntax_penn_tok1_ave,  \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_syntax_penn_tok2_ave'] = pc_fusarolli_align_syntax_penn_tok2_ave,\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_syntax_penn_lemma1_ave'] = pc_fusarolli_align_syntax_penn_lemma1_ave,\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_syntax_penn_lemma2_ave'] = pc_fusarolli_align_syntax_penn_lemma2_ave, \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_lexical_penn_tok1_ave']  = pc_fusarolli_align_lexical_penn_tok1_ave,\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_lexical_penn_tok2_ave']  = pc_fusarolli_align_lexical_penn_tok2_ave,\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_lexical_penn_lemma1_ave'] =pc_fusarolli_align_lexical_penn_lemma1_ave, \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['pc_fusarolli_align_lexical_penn_lemma2_ave'] = pc_fusarolli_align_lexical_penn_lemma2_ave,\n",
    "\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_syntax_penn_tok1_ave'] = cp_fusarolli_align_syntax_penn_tok1_ave \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_syntax_penn_tok2_ave'] = cp_fusarolli_align_syntax_penn_tok2_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_syntax_penn_lemma1_ave'] = cp_fusarolli_align_syntax_penn_lemma1_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_syntax_penn_lemma2_ave'] = cp_fusarolli_align_syntax_penn_lemma2_ave \n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_lexical_penn_tok1_ave'] = cp_fusarolli_align_lexical_penn_tok1_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_lexical_penn_tok2_ave'] = cp_fusarolli_align_lexical_penn_tok2_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_lexical_penn_lemma1_ave'] = cp_fusarolli_align_lexical_penn_lemma1_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['cp_fusarolli_align_lexical_penn_lemma2_ave'] = cp_fusarolli_align_lexical_penn_lemma2_ave\n",
    "            permuted_rand_baseline_resp_avgs[sub][s]['rand_semantic_sim_ave'] = rand_semantic_sim_ave\n",
    "\n",
    "            ###\n",
    "\n",
    "            print(permuted_rand_baseline_resp_avgs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
